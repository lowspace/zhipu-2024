{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "cwd = os.getcwd()\n",
    "os.chdir(cwd)\n",
    "sys.path.append('tools')\n",
    "\n",
    "import chat\n",
    "import parse_data\n",
    "import sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Craft Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\n",
    "\n",
    "prompt_dir = os.path.join(cwd, 'prompt')\n",
    "version = 'v1.1.0'\n",
    "fname = f'sql_generator-stage_1-{version}.md'\n",
    "prompt_fpath = os.path.join(prompt_dir, fname)\n",
    "\n",
    "with open(prompt_fpath, 'r') as f:\n",
    "    prompt_template = ''.join(f.readlines())\n",
    "\n",
    "def make_prompt(data: dict) -> str:\n",
    "\n",
    "    prompt = prompt_template\n",
    "\n",
    "    # \n",
    "    table_finder_res = data['table_finder']['stage_1'][0]['data_source'][0]\n",
    "    try:\n",
    "        del table_finder_res['question']\n",
    "    except:\n",
    "        pass\n",
    "    table = table_finder_res['table']\n",
    "    table_finder_res = json.dumps(table_finder_res, ensure_ascii=False, indent=2)\n",
    "    reg_p = re.compile('<Database and Table>')\n",
    "    prompt = re.sub(reg_p, table_finder_res, prompt)\n",
    "\n",
    "    # \n",
    "    table_fname = f'{table}-with_table_name.md'\n",
    "    table_dir = os.path.join(cwd, 'data' + os.sep + 'table-column')\n",
    "    table_fpath = os.path.join(table_dir, table_fname)\n",
    "    with open(table_fpath,'r') as f:\n",
    "        table_schema = ''.join(f.readlines())\n",
    "    reg_p = re.compile('<Table-Column Schema>')\n",
    "    prompt = re.sub(reg_p, table_schema, prompt)\n",
    "\n",
    "    # \n",
    "    if data['ner']['stage_1']['result']:\n",
    "        ner_res = [i for i in data['ner']['stage_1']['sql'].values() if i][0]\n",
    "        ner_res = [i['result'] for i in ner_res if i['result']][0][0]\n",
    "        ner_res = json.dumps(ner_res, ensure_ascii=False, indent=2)\n",
    "        reg_p = re.compile('<Background Knowledge>')\n",
    "        prompt = re.sub(reg_p, ner_res, prompt)\n",
    "    else:\n",
    "        reg_p = re.compile('<Background Knowledge>')\n",
    "        prompt = re.sub(reg_p, '', prompt)\n",
    "        reg_p = re.compile('## Background Knowledge')\n",
    "        prompt = re.sub(reg_p, '', prompt)\n",
    "\n",
    "    # replace query\n",
    "    query = data['team'][0]['question']\n",
    "    reg_p = re.compile('<Current Query>')\n",
    "    prompt = re.sub(reg_p, query, prompt)\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_path = os.path.join(cwd, 'answer_tmp' + os.sep + 'stage_1-glm_4_plus-table_finder-v2.6.2.json')\n",
    "\n",
    "questions = parse_data.read_json(question_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM-4-Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'glm_4_plus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_tokens': 3911, 'completion_tokens': 174, 'total_tokens': 4085}\n",
      "```json\n",
      "{\n",
      "    \"query\": \"今天是2021年12月24日，创近半年新高的股票有几只？\",\n",
      "    \"sql_cot_reasoning\": \"为了回答这个问题，我们需要查询在2021年12月24日这一天创近半年新高的股票数量。根据表结构，创近半年新高的字段是`IfHighestHPriceRMSix`，其值为1表示创近半年新高。我们需要筛选出在2021年12月24日这一天的记录，并且`IfHighestHPriceRMSix`值为1的记录。使用`COUNT`函数可以统计满足条件的记录数量。\",\n",
      "    \"sql_query\": \"SELECT COUNT(*) FROM CS_StockPatterns WHERE TradingDay LIKE '2021-12-24%' AND IfHighestHPriceRMSix = 1\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "query = make_prompt(questions[1])\n",
    "\n",
    "history = []\n",
    "\n",
    "start_time = time.time()\n",
    "message = chat.create_message(query, history=history, system_prompt=system_prompt, temperature=0.7, top_p=0.9, response_format='text')\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "usage = chat.get_token_usage(message, True)\n",
    "content = chat.get_content(message, True)\n",
    "history = chat.build_history(history, message=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = questions[1]\n",
    "t['sql_generator'] = {}\n",
    "t['sql_generator']['stage_1']= [json.loads(content.strip('`json'))]\n",
    "t['token_usage'] = {}\n",
    "t['token_usage']['sql_generator-stage_1'] = [usage]\n",
    "t['time_usage'] = {}\n",
    "t['time_usage']['sql_generator-stage_1'] = [f\"{execution_time:.2f}s\"]\n",
    "t = [t]\n",
    "\n",
    "saved_path = os.path.join(cwd, 'answer_tmp' + os.sep + f'stage_1-{model}-sql_generator-test-{version}.json')\n",
    "parse_data.write_json(t, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/101 [00:05<09:58,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/101 [01:37<13:36,  9.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 63/101 [07:06<05:29,  8.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 64/101 [07:18<06:01,  9.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 73/101 [08:09<02:47,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 84/101 [09:49<02:42,  9.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 97/101 [11:48<00:34,  8.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [12:18<00:00,  7.31s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "for question in tqdm(questions[:]):\n",
    "    try:\n",
    "        # the first question\n",
    "        query = make_prompt(question)\n",
    "\n",
    "        history = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        message = chat.create_message(query, history=history, system_prompt=system_prompt, temperature=0.7, top_p=0.9, response_format='text')\n",
    "        end_time = time.time()\n",
    "\n",
    "        execution_time = end_time - start_time\n",
    "        usage = chat.get_token_usage(message, False)\n",
    "        content = chat.get_content(message, False)\n",
    "\n",
    "        res = question\n",
    "        res['sql_generator'] = {}\n",
    "        res['sql_generator']['stage_1']= [json.loads(content.strip('`json'))]\n",
    "        res['token_usage']['sql_generator-stage_1'] = [usage]\n",
    "        res['time_usage']['sql_generator-stage_1'] = [f\"{execution_time:.2f}s\"]\n",
    "\n",
    "        answers.append(res)\n",
    "    except:\n",
    "        print(question['tid'])\n",
    "\n",
    "saved_path = os.path.join(cwd, 'answer_tmp' + os.sep + f'stage_1-{model}-sql_generator-{version}.json')\n",
    "parse_data.write_json(answers, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepseek-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'deepseek_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_api = 'sk-ba0f5eed3bea4fa6be16eb33b139c684'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "query = make_prompt(questions[1])\n",
    "\n",
    "client = OpenAI(api_key= deepseek_api, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "start_time = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ],\n",
    "    stream=False,\n",
    "    top_p=0.7,\n",
    "    temperature=0.9\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "response = json.loads(response.to_json())\n",
    "content = response['choices'][0]['message']['content']\n",
    "\n",
    "content = content.strip('`json')\n",
    "usage = response['usage']\n",
    "execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = questions[1]\n",
    "t['sql_generator'] = {}\n",
    "t['sql_generator']['stage_1']= [json.loads(content.strip('`json'))]\n",
    "t['token_usage'] = {}\n",
    "t['token_usage']['sql_generator-stage_1'] = [usage]\n",
    "t['time_usage'] = {}\n",
    "t['time_usage']['sql_generator-stage_1'] = [f\"{execution_time:.2f}s\"]\n",
    "t = [t]\n",
    "\n",
    "saved_path = os.path.join(cwd, 'answer_tmp' + os.sep + f'stage_1-{model}-sql_generator-test-{version}.json')\n",
    "parse_data.write_json(t, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▎     | 44/101 [03:42<04:42,  4.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 58/101 [04:56<04:08,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 59/101 [05:06<04:59,  7.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 60/101 [05:19<05:58,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 73/101 [06:39<03:03,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 92/101 [08:40<01:04,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tttt----92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [09:45<00:00,  5.80s/it]\n"
     ]
    }
   ],
   "source": [
    "answers = []\n",
    "\n",
    "for question in tqdm(questions[:]):\n",
    "    try:\n",
    "        query = make_prompt(question)\n",
    "\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"deepseek-chat\",\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": query},\n",
    "            ],\n",
    "            stream=False,\n",
    "            top_p=0.7,\n",
    "            temperature=0.9\n",
    "        )\n",
    "        end_time = time.time()\n",
    "\n",
    "        response = json.loads(response.to_json())\n",
    "        content = response['choices'][0]['message']['content']\n",
    "\n",
    "        content = content.strip('`json')\n",
    "        usage = response['usage']\n",
    "        execution_time = end_time - start_time\n",
    "\n",
    "        res = question\n",
    "        res['sql_generator'] = {}\n",
    "        res['sql_generator']['stage_1']= [json.loads(content.strip('`json'))]\n",
    "        res['token_usage']['sql_generator-stage_1'] = [usage]\n",
    "        res['time_usage']['sql_generator-stage_1'] = [f\"{execution_time:.2f}s\"]\n",
    "\n",
    "        answers.append(res)\n",
    "    except:\n",
    "        print(question['tid'])\n",
    "        \n",
    "saved_path = os.path.join(cwd, 'answer_tmp' + os.sep + f'stage_1-{model}-sql_generator-{version}.json')\n",
    "parse_data.write_json(answers, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results\n",
    "\n",
    "Compare the stage 1 results => find the differences => get the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "# Path to the folder containing model answer files\n",
    "dir_path = os.path.join(cwd, 'answer_tmp')\n",
    "\n",
    "# List of model names\n",
    "models = ['deepseek_v3', 'glm_4_plus']\n",
    "\n",
    "# Create a dictionary of file paths for each model's JSON file\n",
    "model_files = {model: os.path.join(dir_path, f\"stage_1-{model}-sql_generator-{version}.json\") for model in models}\n",
    "\n",
    "# Dictionary to store the data of each model\n",
    "model_data = {}\n",
    "\n",
    "# Read the JSON data for each model\n",
    "for model, file_path in model_files.items():\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        model_data[model] = json.load(f)\n",
    "\n",
    "# Dictionary to store the data_source for each question id across different models\n",
    "data_sources = {}\n",
    "\n",
    "# Traverse through each model's data to extract the data_source for each question id\n",
    "for model in models:\n",
    "    for entry in copy.deepcopy(model_data[model]):\n",
    "        tmp = entry['sql_generator']['stage_1'][0]['sql_query']\n",
    "        for i in range(len(tmp)):\n",
    "            del tmp[i]['question']\n",
    "        data_sources.setdefault(entry['tid'].replace(' ', ''), {}).update({model: tmp})\n",
    "\n",
    "# Compare the data_source for each question id across models\n",
    "for question_id, sources in data_sources.items():\n",
    "    # Check if the data_source is consistent across models\n",
    "    # Convert each model's data_source to a JSON string (to handle the dictionary comparison)\n",
    "    serialized_sources = {model: json.dumps(ds, sort_keys=True, ensure_ascii=False) for model, ds in sources.items()}\n",
    "    \n",
    "    # If there are any differences in data_source, output the details\n",
    "    if len(set(serialized_sources.values())) > 1:\n",
    "        print(f\"Question ID: {question_id}\")\n",
    "\n",
    "        q_id = int(question_id.split('-')[-1]) - 1\n",
    "        \n",
    "        # Calculate the maximum length of model names to align the output\n",
    "        max_model_length = max(len(model) for model in models)\n",
    "        # Print the data_source for each model, with aligned output\n",
    "        for model in models:\n",
    "            # Left-align model names with the calculated maximum length\n",
    "            print(f\"{model.ljust(max_model_length)}:\")\n",
    "            print('```')\n",
    "            print(json.dumps(model_data[model][q_id]['sql_generator']['stage_1'], indent=2, ensure_ascii=False))\n",
    "            print('```')\n",
    "        \n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
