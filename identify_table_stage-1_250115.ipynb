{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "cwd = os.getcwd()\n",
    "os.chdir(cwd)\n",
    "sys.path.append('tools')\n",
    "\n",
    "import chat\n",
    "import parse_data\n",
    "import embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = 'v2.7.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\n",
    "\n",
    "prompt_dir = os.path.join(cwd, 'prompt')\n",
    "fname = f'table_finder-stage_1-{version}.md'\n",
    "prompt_fpath = os.path.join(prompt_dir, fname)\n",
    "\n",
    "with open(prompt_fpath, 'r') as f:\n",
    "    prompt_template = ''.join(f.readlines())\n",
    "\n",
    "\n",
    "def parse_database_and_table(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Parse the given SQL query to return the database and table names in a dictionary.\n",
    "    \n",
    "    Args:\n",
    "    - query (str): The SQL query to parse.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: A dictionary with 'database' and 'table' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r'FROM\\s+([a-zA-Z0-9_]+)\\.([a-zA-Z0-9_]+)'\n",
    "    match = re.search(pattern, query, re.IGNORECASE)\n",
    "    \n",
    "    if match:\n",
    "        database = match.group(1)\n",
    "        table = match.group(2)\n",
    "        return {'database': database, 'table': table}\n",
    "    \n",
    "    return {}\n",
    "\n",
    "\n",
    "def make_prompt(query: str, ner: dict) -> str:\n",
    "\n",
    "    \"\"\"\n",
    "    ner_res: content from the stage_1\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = prompt_template + query\n",
    "\n",
    "    # ner_result is None\n",
    "    if not ner['result']:\n",
    "        return prompt\n",
    "\n",
    "    ner_content = {}\n",
    "    # assume there is only one NER result\n",
    "    ner_content.update(ner['result'][0])\n",
    "\n",
    "    sql_res = ner['sql']\n",
    "\n",
    "    for k, v in sql_res.items():\n",
    "        for j in v:\n",
    "            if not j['result']:\n",
    "                continue\n",
    "\n",
    "            sql_query = j['query']\n",
    "            # add database and table\n",
    "            ner_content.update(parse_database_and_table(sql_query))\n",
    "            # get market\n",
    "            if ner_content['table'] == 'US_SecuMain':\n",
    "                ner_content['market'] = 'US'\n",
    "            elif ner_content['table'] == 'HK_SecuMain':\n",
    "                ner_content['market'] = 'HK'\n",
    "            else:\n",
    "                ner_content['market'] = 'CN'\n",
    "            ner_content['data_from_table'] = j['result']\n",
    "\n",
    "    # add NER result\n",
    "\n",
    "    ner_str = f\"\\n\\n### **Name Entity Recognition Result**\\n```json\\n{json.dumps(ner_content, ensure_ascii=False,indent=2)}\\n```\"\n",
    "\n",
    "    prompt += ner_str\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_path = os.path.join(cwd, 'answer_tmp' + os.sep + 'stage_1-glm_4_plus-ner-v2.0.0-sql-HF-Post.json')\n",
    "\n",
    "questions = parse_data.read_json(question_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM-4-Plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'glm_4_plus'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = make_prompt(questions[12]['team'][0]['question'], questions[12]['ner']['stage_1'])\n",
    "\n",
    "history = []\n",
    "\n",
    "start_time = time.time()\n",
    "message = chat.create_message(query, history=history, system_prompt=system_prompt, temperature=0.7, top_p=0.9, response_format='text')\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "usage = chat.get_token_usage(message, True)\n",
    "content = chat.get_content(message, True)\n",
    "history = chat.build_history(history, message=message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = questions[12]\n",
    "t['table_finder'] = {}\n",
    "t['table_finder']['stage_1']= [json.loads(content.strip('`json'))]\n",
    "t['token_usage'] = {}\n",
    "t['token_usage']['table_finder-stage_1'] = [usage]\n",
    "t['time_usage'] = {}\n",
    "t['time_usage']['table_finder-stage_1'] = [f\"{execution_time:.2f}s\"]\n",
    "t = [t]\n",
    "\n",
    "saved_path = os.path.join(cwd, 'answer_tmp' + os.sep + f'stage_1-{model}-table_finder-test-{version}.json')\n",
    "parse_data.write_json(t, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for question in tqdm(questions[:]):\n",
    "    try:\n",
    "        # the first question\n",
    "        query = make_prompt(question['team'][0]['question'], question['ner']['stage_1'])\n",
    "\n",
    "        history = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        message = chat.create_message(query, history=history, system_prompt=system_prompt, temperature=0.7, top_p=0.9, response_format='text')\n",
    "        end_time = time.time()\n",
    "\n",
    "        execution_time = end_time - start_time\n",
    "        usage = chat.get_token_usage(message, False)\n",
    "        content = chat.get_content(message, False)\n",
    "\n",
    "        res = question\n",
    "        res['table_finder'] = {}\n",
    "        res['table_finder']['stage_1']= [json.loads(content.strip('`json'))]\n",
    "        res['token_usage']['table_finder-stage_1'] = [usage]\n",
    "        res['time_usage']['table_finder-stage_1'] = [f\"{execution_time:.2f}s\"]\n",
    "\n",
    "        answers.append(res)\n",
    "    except:\n",
    "        print(question['tid'])\n",
    "\n",
    "saved_path = os.path.join(cwd, 'answer_tmp' + os.sep + f'stage_1-{model}-table_finder-{version}.json')\n",
    "parse_data.write_json(answers, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepseek-chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'deepseek_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "deepseek_api = 'sk-ba0f5eed3bea4fa6be16eb33b139c684'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "query = make_prompt(questions[0]['team'][0]['question'], questions[0]['ner']['stage_1'])\n",
    "\n",
    "client = OpenAI(api_key= deepseek_api, base_url=\"https://api.deepseek.com\")\n",
    "\n",
    "start_time = time.time()\n",
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ],\n",
    "    stream=False,\n",
    "    top_p=0.7,\n",
    "    temperature=0.9\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "response = json.loads(response.to_json())\n",
    "content = response['choices'][0]['message']['content']\n",
    "\n",
    "content = content.strip('`json')\n",
    "usage = response['usage']\n",
    "execution_time = end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = questions[0]\n",
    "t['table_finder'] = {}\n",
    "t['table_finder']['stage_1']= [json.loads(content.strip('`json'))]\n",
    "t['token_usage'] = {}\n",
    "t['token_usage']['table_finder-stage_1'] = [usage]\n",
    "t['time_usage'] = {}\n",
    "t['time_usage']['table_finder-stage_1'] = [f\"{execution_time:.2f}s\"]\n",
    "t = [t]\n",
    "\n",
    "saved_path = os.path.join(cwd, 'answer_tmp' + os.sep + f'stage_1-{model}-table_finder-test-{version}.json')\n",
    "parse_data.write_json(t, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "\n",
    "for question in tqdm(questions[:]):\n",
    "    \n",
    "    query = make_prompt(question['team'][0]['question'], question['ner']['stage_1'])\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        stream=False,\n",
    "        top_p=0.7,\n",
    "        temperature=0.9\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    response = json.loads(response.to_json())\n",
    "    content = response['choices'][0]['message']['content']\n",
    "    content = content.strip('`json')\n",
    "    usage = response['usage']\n",
    "    execution_time = end_time - start_time\n",
    "\n",
    "    res = question\n",
    "    res['table_finder'] = {}\n",
    "    res['table_finder']['stage_1']= [json.loads(content.strip('`json'))]\n",
    "    res['token_usage']['table_finder-stage_1'] = [usage]\n",
    "    res['time_usage']['table_finder-stage_1'] = [f\"{execution_time:.2f}s\"]\n",
    "\n",
    "    answers.append(res)\n",
    "\n",
    "saved_path = os.path.join(cwd, 'answer_tmp' + os.sep + f'stage_1-{model}-table_finder-{version}.json')\n",
    "parse_data.write_json(answers, saved_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Results\n",
    "\n",
    "Compare the stage 1 results => find the differences => get the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import copy\n",
    "\n",
    "# Path to the folder containing model answer files\n",
    "dir_path = os.path.join(cwd, 'answer_tmp')\n",
    "\n",
    "# List of model names\n",
    "models = ['deepseek_v3', 'glm_4_plus']\n",
    "\n",
    "# Create a dictionary of file paths for each model's JSON file\n",
    "model_files = {model: os.path.join(dir_path, f\"stage_1-{model}-table_finder-{version}.json\") for model in models}\n",
    "\n",
    "# Dictionary to store the data of each model\n",
    "model_data = {}\n",
    "\n",
    "# Read the JSON data for each model\n",
    "for model, file_path in model_files.items():\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        model_data[model] = json.load(f)\n",
    "\n",
    "# sort data\n",
    "for key in model_data:\n",
    "    model_data[key] = sorted(model_data[key], key=lambda x: x['tid'])\n",
    "\n",
    "# Dictionary to store the data_source for each question id across different models\n",
    "data_sources = {}\n",
    "\n",
    "# Traverse through each model's data to extract the data_source for each question id\n",
    "for model in models:\n",
    "    for entry in copy.deepcopy(model_data[model]):\n",
    "        tmp = entry['table_finder']['stage_1'][0]['data_source']\n",
    "        data_sources.setdefault(entry['tid'].replace(' ', ''), {}).update({model: tmp})\n",
    "\n",
    "# Compare the data_source for each question id across models\n",
    "for question_id, sources in data_sources.items():\n",
    "    # Check if the data_source is consistent across models\n",
    "    # Convert each model's data_source to a JSON string (to handle the dictionary comparison)\n",
    "    results = [v for k, v in sources.items()]\n",
    "    if results[0] == results[1]:\n",
    "        pass\n",
    "    else:\n",
    "        print(f\"Question ID: {question_id}\")\n",
    "\n",
    "        q_id = int(question_id.split('-')[-1]) - 1\n",
    "        \n",
    "        # Calculate the maximum length of model names to align the output\n",
    "        max_model_length = max(len(model) for model in models)\n",
    "        # Print the data_source for each model, with aligned output\n",
    "        for model in models:\n",
    "            # Left-align model names with the calculated maximum length\n",
    "            print(f\"{model.ljust(max_model_length)}:\")\n",
    "            print('```')\n",
    "            for d in model_data[model]:\n",
    "                if d['tid'] == question_id:\n",
    "                    print(json.dumps(d['table_finder']['stage_1'], indent=2, ensure_ascii=False))\n",
    "            print('```')\n",
    "        \n",
    "        print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
